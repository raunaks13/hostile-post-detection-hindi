{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS685 Data Processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnxhyd_wSRTP"
      },
      "source": [
        "# Importing the required resources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtgtYS8kBPAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27edc618-a9f3-4e27-96fd-059dc154aa76"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import ast\n",
        "import string\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8-a-Guexx99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dfb645ae-77f8-4cf3-d29e-0bb26da121f8"
      },
      "source": [
        "!pip install inltk\n",
        "\n",
        "!pip install indic-nlp-library\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "\n",
        "!pip install stanza\n",
        "\n",
        "import inltk\n",
        "import stanza"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting inltk\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/cc/942b7e86043dc9caa3ea967665b30b84527f2a163aaf3f7d14d9afcd7d1a/inltk-0.9-py3-none-any.whl\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from inltk) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from inltk) (20.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from inltk) (1.1.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from inltk) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from inltk) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from inltk) (0.7)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from inltk) (1.18.5)\n",
            "Collecting typing\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/d9/6eebe19d46bd05360c9a9aae822e67a80f9242aabbfc58b641b957546607/typing-3.7.4.3.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from inltk) (4.6.3)\n",
            "Collecting aiohttp>=3.5.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/c6/c518b46d9bf1ae08c1936d82eae4190455ab073bddbb70ddf371211d3151/aiohttp-3.7.2-cp36-cp36m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 8.6MB/s \n",
            "\u001b[?25hCollecting fastai==1.0.57\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/e2/42342ded0385d694e3250e74f43f0dc9a3ff3d5c2241a2ddd98236b5f9de/fastai-1.0.57-py3-none-any.whl (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 16.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from inltk) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from inltk) (1.4.1)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from inltk) (2.2.4)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 19.9MB/s \n",
            "\u001b[?25hCollecting async-timeout>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from inltk) (1.3.2)\n",
            "Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.6/dist-packages (from inltk) (1.0.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from inltk) (7.0.0)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from inltk) (7.352.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->inltk) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->inltk) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->inltk) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->inltk) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->inltk) (3.0.4)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/8b/f4176c06233f7baed99dcb5aefcb010bfbbe769050579adda63083f2c326/yarl-1.6.2-cp36-cp36m-manylinux2014_x86_64.whl (295kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 21.2MB/s \n",
            "\u001b[?25hCollecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (3.7.4.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp>=3.5.4->inltk) (20.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/a0/7c0f5bf1bdcfe88da60d13ba1fead20cb960ae11a355adafae59907d9ae1/multidict-5.0.0-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 24.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.57->inltk) (1.7.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai==1.0.57->inltk) (0.8.1+cu101)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->inltk) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->inltk) (0.10.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (3.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (2.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (50.3.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->inltk) (7.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->fastai==1.0.57->inltk) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (3.4.0)\n",
            "Building wheels for collected packages: typing, idna-ssl\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-cp36-none-any.whl size=26307 sha256=baa8569a30bb7c2ebf213b7ce24c817db1e41960e9f745a087aa4413f890a312\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/04/41/8e1836e79581989c22eebac3f4e70aaac9af07b0908da173be\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3161 sha256=1c1226f5a23340b84c8aff04458c1a7aea2dcc86b3e85f180db12b7bfe26d030\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "Successfully built typing idna-ssl\n",
            "Installing collected packages: typing, multidict, yarl, idna-ssl, async-timeout, aiohttp, fastai, sentencepiece, inltk\n",
            "  Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "Successfully installed aiohttp-3.7.2 async-timeout-3.0.1 fastai-1.0.57 idna-ssl-1.1.0 inltk-0.9 multidict-5.0.0 sentencepiece-0.1.94 typing-3.7.4.3 yarl-1.6.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting indic-nlp-library\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/51/f4e4542a226055b73a621ad442c16ae2c913d6b497283c99cae7a9661e6c/indic_nlp_library-0.71-py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (1.1.4)\n",
            "Collecting morfessor\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->indic-nlp-library) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->indic-nlp-library) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->indic-nlp-library) (1.15.0)\n",
            "Installing collected packages: morfessor, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.71 morfessor-2.0.6\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 133 (delta 0), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (133/133), 149.77 MiB | 18.49 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Checking out files: 100% (28/28), done.\n",
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 1266 (delta 47), reused 51 (delta 24), pack-reused 1178\u001b[K\n",
            "Receiving objects: 100% (1266/1266), 9.56 MiB | 5.92 MiB/s, done.\n",
            "Resolving deltas: 100% (651/651), done.\n",
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/8b/3a9e7a8d8cb14ad6afffc3983b7a7322a3a24d94ebc978a70746fcffc085/stanza-1.1.1-py3-none-any.whl (227kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (50.3.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83N0BzX1xlpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffb56b3b-037f-4fa7-c260-4997dedd5a1f"
      },
      "source": [
        "#INDIC NLP\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "#STANFORD NLP\n",
        "stanza.download('hi') #208 MB file, will take a long time to execute"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 19.8MB/s]                    \n",
            "2020-10-29 08:21:15 INFO: Downloading default packages for language: hi (Hindi)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/hi/default.zip: 100%|██████████| 208M/208M [00:07<00:00, 27.2MB/s]\n",
            "2020-10-29 08:21:27 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgRKgSXNBzUI"
      },
      "source": [
        "# First add the folder at https://drive.google.com/drive/folders/11tcWX3jDWzSLkGhduIonhXpwYMOXfuWQ?usp=sharing to your personal \"My Drive\"\n",
        "train_data = pd.read_csv(\"/content/gdrive/My Drive/CS685/constraint_hindi_train.csv\")\n",
        "valid_data = pd.read_csv(\"/content/gdrive/My Drive/CS685/constraint_hindi_valid.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY3XT4OBgGo6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "d94fb030-c1d1-4e37-b309-b682481108e9"
      },
      "source": [
        "# The output files for this code are present here: \n",
        "# https://drive.google.com/file/d/1hd5zbNqbohhM0b7wzhkvnfpHc-lm06Pp/view?usp=sharing\n",
        "# Add this to the CS685 folder in your My Drive where you had saved the train and test data\n",
        "processed_df_train = pd.read_csv(\"/content/gdrive/My Drive/CS685/processed_df_train.csv\")\n",
        "processed_df_valid = pd.read_csv(\"/content/gdrive/My Drive/CS685/processed_df_valid.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UID</th>\n",
              "      <th>post</th>\n",
              "      <th>tokenised_indic</th>\n",
              "      <th>labels</th>\n",
              "      <th>filtered_indic</th>\n",
              "      <th>lemmatized_posts</th>\n",
              "      <th>filtered_lemmatized_indic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>मेरे देश के हिन्दु बहुत निराले है  कुछ तो पक्क...</td>\n",
              "      <td>['मेरे', 'देश', 'के', 'हिन्दु', 'बहुत', 'निराल...</td>\n",
              "      <td>hate,offensive</td>\n",
              "      <td>['मेरे', 'देश', 'हिन्दु', 'निराले', 'पक्के', '...</td>\n",
              "      <td>मैं देश का हिन्दु बहुत निराला है कुछ तो पक्का ...</td>\n",
              "      <td>['मैं', 'देश', 'हिन्दु', 'निराला', 'पक्का', 'र...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>सरकार हमेशा से किसानों की कमाई को बढ़ाने के लि...</td>\n",
              "      <td>['सरकार', 'हमेशा', 'से', 'किसानों', 'की', 'कमा...</td>\n",
              "      <td>non-hostile</td>\n",
              "      <td>['सरकार', 'हमेशा', 'किसानों', 'कमाई', 'बढ़ाने'...</td>\n",
              "      <td>सरकार हमेशा से किसान का कमाई को बढ़ा का लिए नई...</td>\n",
              "      <td>['सरकार', 'हमेशा', 'किसान', 'कमाई', 'बढ़ा', 'न...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>सुशांत ने जो बिजनेस डील 9 जून को की थी, वो डील...</td>\n",
              "      <td>['सुशांत', 'ने', 'जो', 'बिजनेस', 'डील', '9', '...</td>\n",
              "      <td>non-hostile</td>\n",
              "      <td>['सुशांत', 'बिजनेस', 'डील', '9', 'जून', 'वो', ...</td>\n",
              "      <td>सुशांत ने जो बिजनेस डील 9 जून को कर था COMMA व...</td>\n",
              "      <td>['सुशांत', 'बिजनेस', 'डील', '9', 'जून', 'COMMA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>साले जेएनयू छाप कमिने लोग हिन्दुओं को यह कहते...</td>\n",
              "      <td>['साले', 'जेएनयू', 'छाप', 'कमिने', 'लोग', 'हिन...</td>\n",
              "      <td>defamation,offensive</td>\n",
              "      <td>['साले', 'जेएनयू', 'छाप', 'कमिने', 'लोग', 'हिन...</td>\n",
              "      <td>साला जेएनयू छाप कमिने लोग हिंदू को यह कह है का...</td>\n",
              "      <td>['साला', 'जेएनयू', 'छाप', 'कमिने', 'लोग', 'हिं...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>#unlock4guidelines - अनलॉक-4 के लिए गाइडलाइन्स...</td>\n",
              "      <td>['#', 'unlock4guidelines', '-', 'अनलॉक', '-', ...</td>\n",
              "      <td>non-hostile</td>\n",
              "      <td>['#', 'unlock4guidelines', 'अनलॉक', '4', 'गाइड...</td>\n",
              "      <td>#unlock4guidelines - अनलॉक-4 का लिए गाइडलाइन ज...</td>\n",
              "      <td>['#', 'unlock4guidelines', 'अनलॉक', '4', 'गाइड...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UID  ...                          filtered_lemmatized_indic\n",
              "0    1  ...  ['मैं', 'देश', 'हिन्दु', 'निराला', 'पक्का', 'र...\n",
              "1    2  ...  ['सरकार', 'हमेशा', 'किसान', 'कमाई', 'बढ़ा', 'न...\n",
              "2    3  ...  ['सुशांत', 'बिजनेस', 'डील', '9', 'जून', 'COMMA...\n",
              "3    4  ...  ['साला', 'जेएनयू', 'छाप', 'कमिने', 'लोग', 'हिं...\n",
              "4    5  ...  ['#', 'unlock4guidelines', 'अनलॉक', '4', 'गाइड...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY05oZ2Kvq4N"
      },
      "source": [
        "## Output data description\n",
        "\n",
        "* **post:** original post from training data\n",
        "\n",
        "* **labels:** taken from training data\n",
        "\n",
        "* **tokenised_indic:** tokenised posts using indic nlp library\n",
        "\n",
        "* **filtered_indic:** took posts from tokenised_indic and removed stop words and excess punctuation\n",
        "\n",
        "* **lemmatized_posts:** lemmatized the original posts using stanford nlp\n",
        "\n",
        "* **filtered_lemmatized_indic:** lemmatized original posts, then tokenised them and removed stop words and excess punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDcetUhCFnxa"
      },
      "source": [
        "The code below is only for testing out data preprocessing. Use the dataframe generated above if you want to train models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj-fkLvNxdOq"
      },
      "source": [
        "#Tweet manipulation - data preprocessing\n",
        "\n",
        "#Remove emojis\n",
        "def remove_emojis(tweet):\n",
        "  emojis = re.compile(\"[\"\n",
        "          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "          u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "          u\"\\U00002702-\\U000027B0\"\n",
        "          u\"\\U00002702-\\U000027B0\"\n",
        "          u\"\\U000024C2-\\U0001F251\"\n",
        "          u\"\\U0001f926-\\U0001f937\"\n",
        "          u\"\\U00010000-\\U0010ffff\"\n",
        "          u\"\\u2640-\\u2642\"\n",
        "          u\"\\u2600-\\u2B55\"\n",
        "          u\"\\u200d\"\n",
        "          u\"\\u23cf\"\n",
        "          u\"\\u23e9\"\n",
        "          u\"\\u231a\"\n",
        "          u\"\\ufe0f\"  # dingbats\n",
        "          u\"\\u3030\"\n",
        "                            \"]+\", flags=re.UNICODE)\n",
        "  return emojis.sub(r'', tweet)\n",
        "\n",
        "#Remove URLs and the 'RT' string\n",
        "def remove_url_and_RT(tweet):\n",
        "  return re.sub(r\"http\\S+|RT\", \"\", tweet)\n",
        "\n",
        "#Remove usernames\n",
        "def remove_username(tweet):\n",
        "  return re.sub(r\"@\\S+\",\"\",tweet)\n",
        "\n",
        "#Remove some punctuation\n",
        "def remove_punct(tweet):\n",
        "  tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "  return re.sub(r\"\\n|।|\\|\", \" \", tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2aTzeZJSnQv"
      },
      "source": [
        "# Tokenization + Removing basic punctuation\n",
        "def tokenize(dataset):\n",
        "  n = int(dataset.size/3)\n",
        "  processed = []\n",
        "  for i in range(n):\n",
        "    post = dataset.loc[i][\"Post\"]\n",
        "    post = remove_punct(remove_username(remove_url_and_RT(remove_emojis(post))))\n",
        "    tokenised_indic = indic_tokenize.trivial_tokenize(post)\n",
        "    processed.append([dataset.loc[i][\"Unique ID\"], post, tokenised_indic, dataset.loc[i][\"Labels Set\"]])\n",
        "\n",
        "  return processed\n",
        "\n",
        "processed_df = pd.DataFrame(tokenize(valid_data), columns = ['UID', 'post','tokenised_indic','labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npDTzzdont_J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9afb5c6-03cc-48b5-f43e-6d2284fa78e4"
      },
      "source": [
        "# Loads list of hindi stopwords\n",
        "# Source: https://github.com/taranjeet/hindi-tokenizer/blob/master/stopwords.txt\n",
        "# Add to CS685 folder in My Drive where you saved your data\n",
        "f = open(\"/content/gdrive/My Drive/CS685/hindi_stopwords.txt\")\n",
        "lines = f.readlines()\n",
        "stopwords = []\n",
        "for line in lines:\n",
        "  stopwords.append(line.strip('\\n'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['अत', 'अपना', 'अपनी', 'अपने', 'अभी', 'अंदर', 'आदि', 'आप', 'इत्यादि', 'इन ', 'इनका', 'इन्हीं', 'इन्हें', 'इन्हों', 'इस', 'इसका', 'इसकी', 'इसके', 'इसमें', 'इसी', 'इसे', 'उन', 'उनका', 'उनकी', 'उनके', 'उनको', 'उन्हीं', 'उन्हें', 'उन्हों', 'उस', 'उसके', 'उसी', 'उसे', 'एक', 'एवं', 'एस', 'ऐसे', 'और', 'कई', 'कर', 'करता', 'करते', 'करना', 'करने', 'करें', 'कहते', 'कहा', 'का', 'काफ़ी', 'कि', 'कितना', 'किन्हें', 'किन्हों', 'किया', 'किर', 'किस', 'किसी', 'किसे', 'की', 'कुछ', 'कुल', 'के', 'को', 'कोई', 'कौन', 'कौनसा', 'गया', 'घर', 'जब', 'जहाँ', 'जा', 'जितना', 'जिन', 'जिन्हें', 'जिन्हों', 'जिस', 'जिसे', 'जीधर', 'जैसा', 'जैसे', 'जो', 'तक', 'तब', 'तरह', 'तिन', 'तिन्हें', 'तिन्हों', 'तिस', 'तिसे', 'तो', 'था', 'थी', 'थे', 'दबारा', 'दिया', 'दुसरा', 'दूसरे', 'दो', 'द्वारा', 'न', 'नके', 'नहीं', 'ना', 'निहायत', 'नीचे', 'ने', 'पर', 'पहले', 'पूरा', 'पे', 'फिर', 'बनी', 'बही', 'बहुत', 'बाद', 'बाला', 'बिलकुल', 'भी', 'भीतर', 'मगर', 'मानो', 'मे', 'में', 'यदि', 'यह', 'यहाँ', 'यही', 'या', 'यिह', 'ये', 'रखें', 'रहा', 'रहे', 'ऱ्वासा', 'लिए', 'लिये', 'लेकिन', 'व', 'वग़ैरह', 'वर्ग', 'वह', 'वहाँ', 'वहीं', 'वाले', 'वुह', 'वे', 'सकता', 'सकते', 'सबसे', 'सभी', 'साथ', 'साबुत', 'साभ', 'सारा', 'से', 'सो', 'संग', 'ही', 'हुआ', 'हुई', 'हुए', 'है', 'हैं', 'हो', 'होता', 'होती', 'होते', 'होना', 'होने']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg67rrf5oHpW"
      },
      "source": [
        "# Filters stopwords out from Indic-NLP tokenized representations and saves them to dataframe\n",
        "# Then filters out extra punctuation\n",
        "\n",
        "extra_punctuation = [',', '.', '-', ':', \"'\", '_', '\"', \";\", '(', ')'] # keeping hashtags, exclamation and question marks for further analysis\n",
        "filterer = stopwords + extra_punctuation\n",
        "\n",
        "filtered_posts = []\n",
        "for i in range(len(processed_df)):\n",
        "  list_words = processed_df.loc[i, 'tokenised_indic']\n",
        "  final_list_words = [w for w in list_words if w not in filterer]\n",
        "  filtered_posts.append(final_list_words)\n",
        "\n",
        "processed_df.loc[:, 'filtered_indic'] = filtered_posts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fSASZqD5of_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07fb91c3-9861-40e5-e7c0-1a32fd4f9ca4"
      },
      "source": [
        "# Lemmatization on original post - Will take around 5-10 min\n",
        "nlp = stanza.Pipeline(lang=\"hi\", processors='tokenize,pos,lemma') \n",
        "\n",
        "lemmatized = []\n",
        "for i in range(len(processed_df)):\n",
        "  doc = nlp(processed_df.loc[i, 'post'])\n",
        "  lemma_doc = \"\"\n",
        "  for sent in doc.sentences:\n",
        "    for word in sent.words:\n",
        "      lemma_doc += word.lemma + \" \"\n",
        "  \n",
        "  lemmatized.append(lemma_doc.rstrip())\n",
        "\n",
        "processed_df.loc[:, 'lemmatized_posts'] = lemmatized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-29 08:24:40 INFO: Loading these models for language: hi (Hindi):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | hdtb    |\n",
            "| pos       | hdtb    |\n",
            "| lemma     | hdtb    |\n",
            "=======================\n",
            "\n",
            "2020-10-29 08:24:40 INFO: Use device: cpu\n",
            "2020-10-29 08:24:40 INFO: Loading: tokenize\n",
            "2020-10-29 08:24:40 INFO: Loading: pos\n",
            "2020-10-29 08:24:41 INFO: Loading: lemma\n",
            "2020-10-29 08:24:41 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_pebEcVGSaD"
      },
      "source": [
        "# Take the lemmatized posts and tokenize them. Then remove stopwords.\n",
        "filtered_lemmatized = []\n",
        "for i in range(len(processed_df)):\n",
        "  post = processed_df.loc[i, 'lemmatized_posts']\n",
        "  post = remove_punct(remove_username(remove_url_and_RT(remove_emojis(post))))\n",
        "  tokenised_indic = indic_tokenize.trivial_tokenize(post)\n",
        "  final_list_words = [w for w in tokenised_indic if w not in filterer]\n",
        "  filtered_lemmatized.append(final_list_words)\n",
        "\n",
        "processed_df.loc[:, 'filtered_lemmatized_indic'] = filtered_lemmatized\n",
        "processed_df.to_csv(\"/content/gdrive/My Drive/CS685/processed_df_valid.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H5Sd2qwE9sL"
      },
      "source": [
        "# Baseline RNN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiGpb9FRIfrf"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.data import Field, BucketIterator\n",
        "import torch.optim as optim\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3sOA3IBFOWA"
      },
      "source": [
        "def convert_labels(df):\n",
        "    df[['fake','hate','offensive','defamation','non-hostile']] = 0\n",
        "    for inx, row in df.iterrows():\n",
        "        for label in row['labels'].split(','):\n",
        "            df.loc[inx, label]=1\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxSwPXO8IGag"
      },
      "source": [
        "rnn_df = convert_labels(processed_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMe8X9h0JIjo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "709f5a5a-5b8e-45a6-a345-98313bd0d86e"
      },
      "source": [
        "rnn_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UID</th>\n",
              "      <th>post</th>\n",
              "      <th>tokenised_indic</th>\n",
              "      <th>labels</th>\n",
              "      <th>filtered_indic</th>\n",
              "      <th>lemmatized_posts</th>\n",
              "      <th>filtered_lemmatized_indic</th>\n",
              "      <th>fake</th>\n",
              "      <th>hate</th>\n",
              "      <th>offensive</th>\n",
              "      <th>defamation</th>\n",
              "      <th>non-hostile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>मेरे देश के हिन्दु बहुत निराले है  कुछ तो पक्क...</td>\n",
              "      <td>['मेरे', 'देश', 'के', 'हिन्दु', 'बहुत', 'निराल...</td>\n",
              "      <td>hate,offensive</td>\n",
              "      <td>['मेरे', 'देश', 'हिन्दु', 'निराले', 'पक्के', '...</td>\n",
              "      <td>मैं देश का हिन्दु बहुत निराला है कुछ तो पक्का ...</td>\n",
              "      <td>['मैं', 'देश', 'हिन्दु', 'निराला', 'पक्का', 'र...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>सरकार हमेशा से किसानों की कमाई को बढ़ाने के लि...</td>\n",
              "      <td>['सरकार', 'हमेशा', 'से', 'किसानों', 'की', 'कमा...</td>\n",
              "      <td>non-hostile</td>\n",
              "      <td>['सरकार', 'हमेशा', 'किसानों', 'कमाई', 'बढ़ाने'...</td>\n",
              "      <td>सरकार हमेशा से किसान का कमाई को बढ़ा का लिए नई...</td>\n",
              "      <td>['सरकार', 'हमेशा', 'किसान', 'कमाई', 'बढ़ा', 'न...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>सुशांत ने जो बिजनेस डील 9 जून को की थी, वो डील...</td>\n",
              "      <td>['सुशांत', 'ने', 'जो', 'बिजनेस', 'डील', '9', '...</td>\n",
              "      <td>non-hostile</td>\n",
              "      <td>['सुशांत', 'बिजनेस', 'डील', '9', 'जून', 'वो', ...</td>\n",
              "      <td>सुशांत ने जो बिजनेस डील 9 जून को कर था COMMA व...</td>\n",
              "      <td>['सुशांत', 'बिजनेस', 'डील', '9', 'जून', 'COMMA...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>साले जेएनयू छाप कमिने लोग हिन्दुओं को यह कहते...</td>\n",
              "      <td>['साले', 'जेएनयू', 'छाप', 'कमिने', 'लोग', 'हिन...</td>\n",
              "      <td>defamation,offensive</td>\n",
              "      <td>['साले', 'जेएनयू', 'छाप', 'कमिने', 'लोग', 'हिन...</td>\n",
              "      <td>साला जेएनयू छाप कमिने लोग हिंदू को यह कह है का...</td>\n",
              "      <td>['साला', 'जेएनयू', 'छाप', 'कमिने', 'लोग', 'हिं...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>#unlock4guidelines - अनलॉक-4 के लिए गाइडलाइन्स...</td>\n",
              "      <td>['#', 'unlock4guidelines', '-', 'अनलॉक', '-', ...</td>\n",
              "      <td>non-hostile</td>\n",
              "      <td>['#', 'unlock4guidelines', 'अनलॉक', '4', 'गाइड...</td>\n",
              "      <td>#unlock4guidelines - अनलॉक-4 का लिए गाइडलाइन ज...</td>\n",
              "      <td>['#', 'unlock4guidelines', 'अनलॉक', '4', 'गाइड...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UID  ... non-hostile\n",
              "0    1  ...           0\n",
              "1    2  ...           1\n",
              "2    3  ...           1\n",
              "3    4  ...           0\n",
              "4    5  ...           1\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYyh1F_WdtqP"
      },
      "source": [
        "# Augmentation\n",
        "\n",
        "Inltk augmentation is outdated so will try manual augmentation.\n",
        "Will start working with synonyms now.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g6J1AZ_qRKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3ee9481-4614-4917-ed97-eb4f9f826e6e"
      },
      "source": [
        "! pip install pyiwn\n",
        "import pyiwn \n",
        "iwn = pyiwn.IndoWordNet(lang=pyiwn.Language.HINDI) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyiwn\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/80/55d7412ed602071de200b74d75f7cba8c9b4fe6a2f3df33c6d5cd0e6cb83/pyiwn-0.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pyiwn) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pyiwn) (1.1.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pyiwn) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pyiwn) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pyiwn) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pyiwn) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->pyiwn) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pyiwn) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas->pyiwn) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->pyiwn) (1.15.0)\n",
            "Installing collected packages: pyiwn\n",
            "Successfully installed pyiwn-0.0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-11-12:05:01:02,654 INFO     [helpers.py:20] Downloading IndoWordNet data of size ~31 MB...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-11-12:05:01:06,577 INFO     [helpers.py:43] Extracting /root/iwn_data.tar.gz into /root...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-11-12:05:01:07,795 INFO     [helpers.py:48] Removing temporary zip file from /root/iwn_data.tar.gz\n",
            "2020-11-12:05:01:07,801 INFO     [helpers.py:51] IndoWordNet data successfully downloaded at /root/iwn_data\n",
            "2020-11-12:05:01:07,805 INFO     [iwn.py:43] Loading hindi language synsets...\n",
            "2020-11-12:05:01:08,723 INFO     [utils.py:141] NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSQHM_Vg3OUS"
      },
      "source": [
        "from ast import literal_eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjQEBD-q6vmZ"
      },
      "source": [
        "Augmentation example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c-EP4lzvKbi"
      },
      "source": [
        "augmented2 = []\n",
        "for i in range(len(processed_df_train)):\n",
        "    augmented2.append([processed_df_train.loc[i,'UID'],processed_df_train.loc[i,'post'],processed_df_train.loc[i,'labels']])\n",
        "\n",
        "    words = list(filter(('').__ne__, processed_df_train.loc[i,'post'].split(' ')))\n",
        "    words_new = list(filter(('').__ne__, processed_df_train.loc[i,'post'].split(' ')))\n",
        "    \n",
        "    '''\n",
        "    Augmentation style 2 where the count is scaled according to the original frequency:\n",
        "    '''\n",
        "    # if processed_df_train.loc[i,'labels'].find('fake') is not -1:\n",
        "    #     count = 2\n",
        "    # elif (processed_df_train.loc[i,'labels'].find('hate') is not -1) or (processed_df_train.loc[i,'labels'].find('offensive') is not -1):\n",
        "    #     count = 3\n",
        "    # elif processed_df_train.loc[i,'labels'].find('defamation') is not -1:\n",
        "    #     count = 4\n",
        "    # else:\n",
        "    #     count = 1\n",
        "\n",
        "    '''\n",
        "    Augmentation style 1 where the count is scaled to 2 for all classes i.e. we double the dataset\n",
        "    '''\n",
        "    count = 2\n",
        "\n",
        "    for j in range(1,count):\n",
        "        for k in range(len(words)):\n",
        "            try:\n",
        "                iwn.synsets(words[k], pos=pyiwn.PosTag.NOUN)[0]\n",
        "            except:\n",
        "                continue\n",
        "            \n",
        "            synonyms = iwn.synsets(words[k], pos=pyiwn.PosTag.NOUN)[0].lemma_names() #Finding all synonyms\n",
        "            if (len(synonyms)>j):\n",
        "                words_new[k] = synonyms[j] #replacing the word\n",
        "\n",
        "        new = ''\n",
        "        for word in words_new:\n",
        "            new = new + word + ' '\n",
        "        augmented2.append([processed_df_train.loc[i,'UID'],new,processed_df_train.loc[i,'labels']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHOMSpErfP2W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "593ada9a-33fa-4e72-e4de-922bdc8a20e6"
      },
      "source": [
        "augmented2_df = pd.DataFrame(augmented2, columns = ['UID', 'post','labels'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UID</th>\n",
              "      <th>post</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>मेरे देश के हिन्दु बहुत निराले है  कुछ तो पक्क...</td>\n",
              "      <td>hate,offensive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>मेरे राष्ट्र के हिन्दु कई निराले है कुछ तो पक्...</td>\n",
              "      <td>hate,offensive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>सरकार हमेशा से किसानों की कमाई को बढ़ाने के लि...</td>\n",
              "      <td>non-hostile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>शासन हमेशा से किसानों पेंट आमदनी को बढ़ाने के ...</td>\n",
              "      <td>non-hostile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>सुशांत ने जो बिजनेस डील 9 जून को की थी, वो डील...</td>\n",
              "      <td>non-hostile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11451</th>\n",
              "      <td>5726</td>\n",
              "      <td>जॉब गंवा चुके दोस्त पेंट मदद: नक़दी के बजाए भे...</td>\n",
              "      <td>non-hostile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11452</th>\n",
              "      <td>5727</td>\n",
              "      <td>बंगाल में हिन्दू मरे हैं इसलिए मुझे कोई फर्क न...</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11453</th>\n",
              "      <td>5727</td>\n",
              "      <td>पश्चिम बंगाल मेंमें हिन्दू मरे हैं इसलिए मुझे ...</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11454</th>\n",
              "      <td>5728</td>\n",
              "      <td>रक्षा मंत्री  ने पूर्व राष्ट्रपति #PranabMukhe...</td>\n",
              "      <td>non-hostile</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11455</th>\n",
              "      <td>5728</td>\n",
              "      <td>बचाव मन्त्री ने पूर्व प्रेज़िडेंट #PranabMukhe...</td>\n",
              "      <td>non-hostile</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11456 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        UID                                               post          labels\n",
              "0         1  मेरे देश के हिन्दु बहुत निराले है  कुछ तो पक्क...  hate,offensive\n",
              "1         1  मेरे राष्ट्र के हिन्दु कई निराले है कुछ तो पक्...  hate,offensive\n",
              "2         2  सरकार हमेशा से किसानों की कमाई को बढ़ाने के लि...     non-hostile\n",
              "3         2  शासन हमेशा से किसानों पेंट आमदनी को बढ़ाने के ...     non-hostile\n",
              "4         3  सुशांत ने जो बिजनेस डील 9 जून को की थी, वो डील...     non-hostile\n",
              "...     ...                                                ...             ...\n",
              "11451  5726  जॉब गंवा चुके दोस्त पेंट मदद: नक़दी के बजाए भे...     non-hostile\n",
              "11452  5727  बंगाल में हिन्दू मरे हैं इसलिए मुझे कोई फर्क न...            fake\n",
              "11453  5727  पश्चिम बंगाल मेंमें हिन्दू मरे हैं इसलिए मुझे ...            fake\n",
              "11454  5728  रक्षा मंत्री  ने पूर्व राष्ट्रपति #PranabMukhe...     non-hostile\n",
              "11455  5728  बचाव मन्त्री ने पूर्व प्रेज़िडेंट #PranabMukhe...     non-hostile\n",
              "\n",
              "[11456 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ytVCB34r236"
      },
      "source": [
        "augmented2_df.to_csv(\"/content/gdrive/My Drive/CS685/augmented2_train.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}