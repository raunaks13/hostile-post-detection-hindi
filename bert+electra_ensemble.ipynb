{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"monsoon-nlp/hindi-tpu-electra\")\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"monsoon-nlp/hindi-bert\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df = pd.read_csv(\"../data/constraint_hindi_train.csv\")\n",
    "taglist = list()\n",
    "for i in range(len(df)):\n",
    "    tags = df.iloc[i][\"Labels Set\"].split(\",\")\n",
    "    for tag in tags:\n",
    "        if(tag not in taglist):\n",
    "            taglist.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datapath):\n",
    "        \n",
    "        self.df = pd.read_csv(datapath)\n",
    "    def __len__(self):\n",
    "        return(len(self.df))\n",
    "    def __getitem__(self,idx):\n",
    "        sentence = self.df.iloc[idx][\"Post\"]\n",
    "        encoding = tokenizer1.encode_plus(sentence,add_special_tokens=True,max_length=64,truncation=True,pad_to_max_length=True,return_attention_mask=True)\n",
    "        electra_input_ids = encoding['input_ids']\n",
    "        electra_attention_mask = encoding['attention_mask']\n",
    "        labels = np.zeros(5)\n",
    "        tags = self.df.iloc[idx][\"Labels Set\"].split(\",\")\n",
    "        for tag in tags:\n",
    "            labels[taglist.index(tag)] = 1\n",
    "        \n",
    "        sentence = self.df.iloc[idx][\"Post\"]\n",
    "        encoding = tokenizer2.encode_plus(sentence,add_special_tokens=True,max_length=64,truncation=True,pad_to_max_length=True,return_attention_mask=True)\n",
    "        bert_input_ids = encoding['input_ids']\n",
    "        bert_attention_mask = encoding['attention_mask']\n",
    "        \n",
    "        return np.array(electra_input_ids), np.array(electra_attention_mask),np.array(bert_input_ids), np.array(bert_attention_mask), labels\n",
    "trainset = Dataset(\"../data/constraint_hindi_train.csv\")\n",
    "valset = Dataset(\"../data/constraint_hindi_valid.csv\")\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size = 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class electra_model(nn.Module): ## CNN + Linear (single linear)\n",
    "    def __init__(self):\n",
    "        super(electra_model, self).__init__()\n",
    "        # self.bert = AutoModel.from_pretrained(\"monsoon-nlp/hindi-bert\", output_hidden_states=True)\n",
    "        self.bert = AutoModel.from_pretrained(\"monsoon-nlp/hindi-tpu-electra\", output_hidden_states=True)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(3, 768), padding=(1,0)) for i in range(12)])\n",
    "        self.pools = nn.ModuleList([nn.MaxPool1d(kernel_size=64) for i in range(12)])\n",
    "        self.linear = nn.Linear(in_features=12, out_features=5)\n",
    "    def forward(self,sent,mask):\n",
    "        seq = self.bert(sent,mask)[1] # len 13\n",
    "        hidden_states = seq[1:] # len 12, each (64, 64, bert_hidden_size)\n",
    "        cnn_out = torch.empty(0).to(\"cuda:1\")\n",
    "        for i, state in enumerate(hidden_states):\n",
    "            state = torch.unsqueeze(state, dim=1) # (64, 1, 64, bert_hidden_size)\n",
    "            out = self.convs[i](state) # (64, 1, 64, 1)\n",
    "            out = out.squeeze(dim=3)\n",
    "            out = self.pools[i](out) # (64, 1, 1)\n",
    "            out = out.squeeze(dim=2) # (64, 1)\n",
    "            cnn_out = torch.cat([cnn_out, out], dim=1)\n",
    "\n",
    "        seq = self.linear(cnn_out)\n",
    "        return seq\n",
    "class bert_model(nn.Module): ## Linear on bert (single linear) only CLS token\n",
    "    def __init__(self):\n",
    "        super(bert_model, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"monsoon-nlp/hindi-bert\")\n",
    "        self.linear = nn.Linear(in_features=256, out_features=5)\n",
    "    def forward(self,sent,mask):\n",
    "        seq = self.bert(sent,mask)[0]\n",
    "#         seq = torch.mean(seq, dim = 1).squeeze()\n",
    "        seq = torch.transpose(seq,0,1)[0]\n",
    "        seq = self.linear(seq)\n",
    "        return seq\n",
    "    \n",
    "class ensemble(nn.Module): ## Linear on bert (single linear) only CLS token\n",
    "    def __init__(self):\n",
    "        super(ensemble, self).__init__()\n",
    "        self.bert = bert_model()\n",
    "        self.electra = electra_model()\n",
    "        self.linear = nn.Linear(in_features=10,out_features=5,bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self,electra_sent,electra_mask,bert_sent,bert_mask):\n",
    "        bert_op = self.bert(bert_sent.to(\"cuda:0\"),bert_mask.to(\"cuda:0\"))\n",
    "        electra_op = self.electra(electra_sent.to(\"cuda:1\"),electra_mask.to(\"cuda:1\"))\n",
    "        op = 0.5*bert_op.to(\"cuda:1\") + 0.5*electra_op\n",
    "        op = self.sigmoid(op)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in trainloader:\n",
    "#     break\n",
    "# model = ensemble()\n",
    "# model.to(device)\n",
    "# batch[0],batch[1],batch[2],batch[3],batch[4]=batch[0].to(device),batch[1].to(device),batch[2].to(device),batch[3].to(device),batch[4].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ensemble()\n",
    "model.bert = model.bert.to(\"cuda:0\")\n",
    "model.electra = model.electra.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c55ddeb46784ffa98a5f2a280ca019b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/sagnik/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss at epoch 0:0.4295816467810794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/sagnik/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss at epoch 1:0.375504814244522\n",
      "val loss at epoch 2:0.3356429696744526\n",
      "val loss at epoch 3:0.30946765169409435\n",
      "val loss at epoch 4:0.29446831965214965\n",
      "val loss at epoch 5:0.28390639987756966\n",
      "val loss at epoch 6:0.27358798375847015\n",
      "val loss at epoch 7:0.26562477448213334\n",
      "val loss at epoch 8:0.26218325118781166\n",
      "val loss at epoch 9:0.2574071741459659\n",
      "val loss at epoch 10:0.25106937630370757\n",
      "val loss at epoch 11:0.2484915204421972\n",
      "val loss at epoch 12:0.24276532160644174\n",
      "val loss at epoch 13:0.23934591753217685\n",
      "val loss at epoch 14:0.233703313525146\n",
      "val loss at epoch 15:0.2307493599446355\n",
      "val loss at epoch 16:0.23143283969351766\n",
      "val loss at epoch 17:0.22738309913175928\n",
      "val loss at epoch 18:0.22675428951375012\n",
      "val loss at epoch 19:0.22827028890763915\n",
      "val loss at epoch 20:0.2227656919372517\n",
      "val loss at epoch 21:0.22354802346826244\n",
      "val loss at epoch 22:0.22320261383406204\n",
      "val loss at epoch 23:0.2220269656733239\n",
      "val loss at epoch 24:0.2220402074255435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = list()\n",
    "val_losses=list()\n",
    "val_accuracy=list()\n",
    "loss = nn.BCELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
    "for epoch in tqdm(range(25)):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    num_batches = 0\n",
    "    ## main model training loop\n",
    "    for batch in (trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch[0],batch[1],batch[2],batch[3])\n",
    "        labels = batch[4].to(\"cuda:1\")\n",
    "        loss_value = loss(logits.float(),labels.float())\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss_value.item()\n",
    "        num_batches +=1\n",
    "    train_losses.append(total_loss/num_batches)\n",
    "#     if(epoch%1==0):\n",
    "#         print(\"train loss at epoch \" + str(epoch) + \":\" + str(total_loss/num_batches))\n",
    "\n",
    "    model.eval()\n",
    "    total_loss=0\n",
    "    num_batches = 0\n",
    "    ## validation loss and accuracy calculation\n",
    "    for batch in (valloader):\n",
    "        logits = model(batch[0],batch[1],batch[2],batch[3])\n",
    "        labels = batch[4].to(\"cuda:1\")\n",
    "\n",
    "        loss_value = loss(logits.float(),labels.float())\n",
    "        total_loss+=loss_value.item()\n",
    "        num_batches +=1\n",
    "    val_losses.append(total_loss/num_batches)\n",
    "    if(epoch%1==0):\n",
    "        print(\"val loss at epoch \" + str(epoch) + \":\" + str(total_loss/num_batches))\n",
    "        \n",
    "    logits_list = np.zeros((1,5))\n",
    "    labels_list = np.zeros((1,5))\n",
    "    for batch in (valloader):\n",
    "        logits = model(batch[0],batch[1],batch[2],batch[3]).to(\"cpu\").detach().numpy() > 0.5\n",
    "        labels = batch[4].to(\"cpu\")\n",
    "    \n",
    "        logits_list = np.concatenate((logits_list,logits),0)\n",
    "        labels_list = np.concatenate((labels_list,labels),0)\n",
    "    val_accuracy.append(f1_score(labels_list[1:], logits_list[1:], average='weighted'))\n",
    "#     if(epoch%10==0):\n",
    "#         print(\"val accuracy after epoch \"+str(epoch)+ \" : \" + str(f1_score(labels_list[1:], logits_list[1:], average='weighted')))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbbfcf5a6d8>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/sagnik/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86483720167801\n",
      "0.744596312778131\n",
      "0.7835380390803829\n",
      "0.7718007980719844\n",
      "0.6711864406779661\n",
      "0.705388619241328\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "\n",
    "logits_list = np.zeros((1,5))\n",
    "labels_list = np.zeros((1,5))\n",
    "model.eval()\n",
    "for batch in (trainloader):\n",
    "    logits = model(batch[0],batch[1],batch[2],batch[3]).to(\"cpu\").detach().numpy() > 0.5\n",
    "    labels = batch[4].numpy()\n",
    "    \n",
    "    logits_list = np.concatenate((logits_list,logits),0)\n",
    "    labels_list = np.concatenate((labels_list,labels),0)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "for i in range(logits_list.shape[0]):\n",
    "    for j in range(logits_list.shape[1]):\n",
    "        if(logits_list[i][2]==1 and j!=2):\n",
    "            logits_list[i][j]=0\n",
    "print(precision_score(labels_list[1:], logits_list[1:], average='weighted'))\n",
    "print(recall_score(labels_list[1:], logits_list[1:], average='weighted'))\n",
    "print(f1_score(labels_list[1:], logits_list[1:], average='weighted'))\n",
    "\n",
    "\n",
    "logits_list = np.zeros((1,5))\n",
    "labels_list = np.zeros((1,5))\n",
    "model.eval()\n",
    "for batch in (valloader):\n",
    "    logits = model(batch[0],batch[1],batch[2],batch[3]).to(\"cpu\").detach().numpy() > 0.5\n",
    "    labels = batch[4].numpy()\n",
    "    \n",
    "    logits_list = np.concatenate((logits_list,logits),0)\n",
    "    labels_list = np.concatenate((labels_list,labels),0)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "for i in range(logits_list.shape[0]):\n",
    "    for j in range(logits_list.shape[1]):\n",
    "        if(logits_list[i][2]==1 and j!=2):\n",
    "            logits_list[i][j]=0\n",
    "print(precision_score(labels_list[1:], logits_list[1:], average='weighted'))\n",
    "print(recall_score(labels_list[1:], logits_list[1:], average='weighted'))\n",
    "print(f1_score(labels_list[1:], logits_list[1:], average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[675.  27.]\n",
      " [ 81.  29.]]\n",
      "[[677.  32.]\n",
      " [ 58.  45.]]\n",
      "[[369.   8.]\n",
      " [ 27. 408.]]\n",
      "[[732.   3.]\n",
      " [ 76.   1.]]\n",
      "[[615.  37.]\n",
      " [ 49. 111.]]\n"
     ]
    }
   ],
   "source": [
    "def print_conf(y_true,y_pred,i):\n",
    "    y_true = labels_list[:,i]\n",
    "    y_pred = logits_list[:,i]\n",
    "    matrix_hate = np.zeros((2,2))\n",
    "    for i in range(len(y_true)):\n",
    "        true = int(y_true[i])\n",
    "        pred = int(y_pred[i])\n",
    "        matrix_hate[true][pred]+=1\n",
    "    print(matrix_hate)\n",
    "##coordinate 0 hate\n",
    "for i in range(5):\n",
    "    y_true = labels_list[:,i]\n",
    "    y_pred = logits_list[:,i]\n",
    "    print_conf(y_true,y_pred,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec  4 19:45:58 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  On   | 00000000:05:00.0 Off |                  N/A |\n",
      "| 31%   52C    P0    79W / 250W |     18MiB / 11177MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  On   | 00000000:06:00.0 Off |                  N/A |\n",
      "| 27%   42C    P8     9W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  On   | 00000000:09:00.0 Off |                  N/A |\n",
      "| 24%   43C    P8    11W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  On   | 00000000:0A:00.0 Off |                  N/A |\n",
      "| 23%   35C    P8    10W / 250W |      2MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1633      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A      1729      G   /usr/bin/gnome-shell                6MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
